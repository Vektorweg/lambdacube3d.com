% Introduction to LambdaCube 3D


Scope
=====

LambdaCube 3D is a domain specific language for programming the GPU (graphics processing unit).
LambdaCube 3D is a Haskell-like purely functional language.

Currently LambdaCube 3D is intended for implement the part of the program which is responsible for 2D or 3D computer graphics.
Other parts of the program (e.g. networking, application logic) can be implemented in Haskell, C++ or JavaScript.
LambdaCube 3D can be used for desktop and mobile applications, either in standalone applications or in browsers too.


Related projects
================

You may want to skip this section if you have not heard of the following projects.

OpenGL
------

LambdaCube 3D is an alternative to using OpenGl.
LambdaCube 3D is functional and statically typed as opposed to OpenGl's inherently imperative style.
In any OpenGL bindings a dedicated language, the shader language should be used to implement the shaders (the GPU subprograms).
In LambdaCube 3D, shaders are implemented as regular functions.

GPipe
-----

GPipe is the most related project to LambdaCube 3D, with similar goals. The main difference is that
GPipe is a Haskell library whilst LambdaCube 3D is a standalone Haskell-like language.
See also LambdaCube 3D EDSL below.

LambdaCube 3D EDSL
------------------

LambdaCube 3D EDSL (embedded domain specific language) is a Haskell library, whilst
LambdaCube 3D DSL (abberviated as LambdaCube 3D) is a standalone Haskell-like language with its own compiler and libraries.
Currently the EDSL version is maintained and the DSL version is actively developed.
The DSL version has less syntactic noise. Other pros and cons can be found [here](http://lambdacube3d.com/questions#why-are-you-developing-a-separate-dsl-instead-of-a-haskell-edsl).

LambdaCube 3D EDSL is quite similar to GPipe. The main difference is that GPipe has more expressive power
(it supports dynamic pipelines) whilst there is more optimization opportunity in the LambdaCube 3D EDSL.

Luminance
---------

Luminance is a Haskell EDSL for GPU graphics.
In Luminance, shaders can only be implemented in the C-like shader language.


Overview
========

Basic terminology
-----------------

**Graphics engine** is the part of an application which is responsible for the GPU graphics.
LambdaCube 3D is a DSL for impelementing graphics engines.

From a purely functional point of view, the graphics engine is a pure function.
We call this function the **pipeline**.

A **frame** is the output of the pipeline.
A frame is something which can be put on the screen.

**Rendering** is the computation of one frame given the pipeline and its input.  

Pipelines can be assembled from smaller parts.  

**Multi-pass rendering** is rendering with a pipeline which is a composition of several pipelines where
the output of a pipeline is part of the input of another pipeline. LambdaCube 3D supports multi-pass rendering.

A **dynamic pipeline** is a composed pipeline in which the composition depends on the pipeline input.  
Currently LambdaCube 3D does not support dynamic pipelines. This limitation is planned to be lifted.
Until then a workaround is to use several pipelines and the application logic
chooses at every rendering the right pipeline depending on the input.
(If a predefined set of pipelines is not enough, it is possible to create pipelines on the fly.)

Pipeline inputs
---------------

There are limitations on possible pipeline inputs, which reflects the fact that GPUs are specialised hardware.
The current limitations are inherited from OpenGL 3.3.

The input of a pipeline consists of zero or more vertex streams, zero or more uniforms and zero or more textures.

A **uniform** is a piece of data with a statically known size.
Supported uniform types are `Bool`, `Int`, `Float`, `Vec 3 Bool`, `Mat 4 4 Float`, etc.
For example, a camera position can be part of the input as a 4 dimensional `Float` matrix uniform.
The terminology 'uniform' comes from the fact that uniforms are constant values during the rendering.

A **texture** is a uniform with type `Texture`.
A `Texture` is an image with sampling configurations (discussed later).
Currently textures are treated specially in LambdadCube 3D but this is planned to be changed.

A **vertex stream** is a vector whose elements are **points**, **lines** or **triangles**.
A point consits of one vertex, a line consits of two vertices, and a triangle consists of three vertices.

A **vertex** is a tuple of attributes.

An **attribute** is a piece of data with a statically known size.
Supported attribute types are for example `Float`, `Bool` or `Vec 3 Float`.
Typical attribute are the spatial position or the color of a vertex.

For example, the vertex stream of a tetrahedron looks something like

~~~~~ {.haskell}
{pos: V3 0 0 0} {pos: V3 1 0 0} {pos: V3 0 1 0}   -- 1st triangle
{pos: V3 1 0 0} {pos: V3 0 1 0} {pos: V3 0 0 1}   -- 2nd triangle
{pos: V3 0 1 0} {pos: V3 0 0 1} {pos: V3 0 0 0}   -- 3rd triangle
{pos: V3 0 0 1} {pos: V3 0 0 0} {pos: V3 0 0 1}   -- 4th triangle
~~~~~

The vertex stream of a tetrahedron with an additional `q` Boolean attribute may looks something like

~~~~~ {.haskell}
{pos: V3 0 0 0, q: True } {pos: V3 1 0 0, q: True } {pos: V3 0 1 0, q: True }   -- 1st triangle
{pos: V3 1 0 0, q: True } {pos: V3 0 1 0, q: True } {pos: V3 0 0 1, q: False}   -- 2nd triangle
{pos: V3 0 1 0, q: False} {pos: V3 0 0 1, q: True } {pos: V3 0 0 0, q: True }   -- 3rd triangle
{pos: V3 0 0 1, q: False} {pos: V3 0 0 0, q: True } {pos: V3 0 0 1, q: True }   -- 4th triangle
~~~~~

Note that the representation of the vertex stream is hidden in LambdaCube 3D. Usually the vertex stream is provided as a
flattened stream of vertices by the application.


Frames and images
-----------------

Frames are the main storage units which accumulate the results of intermediate computations.

A frame consists of several images of the same dimension.

An **image** is a two dimensional array of values.

There are different kind of images depending on the role the image plays.

A **color image** is to store color values. Supported color values are `Float`, and `Float` vectors with dimension at most 4.
For example, a color image of `Float` values can be used to store a grayscale image, but the `Float` values can be interpreted in any
color space, or they can be used to a totally different purpose too.

The `emptyColorImage` function creates an empty color image given a color value:

~~~~~ {.haskell}
emptyColorImage navy
~~~~~

A **depth image** is to store depth values. A depth value has type `Float` and it is usually used to store the distance between the
viewpoint and the object shown on a color image.
In LambdaCube 3D, a depth image has different type than a color image with `Float` values because graphics hardware has dedicated
operations for depth images.

The `emptyDepthImage` function creates an empty depth image given a depth value:

~~~~~ {.haskell}
emptyDepthImage 1000
~~~~~

A **stencil image** is to used for masking (discussed later).

### Creating frames from images

The `imageFrame` function creates a frame given a tuple of images:

~~~~~ {.haskell}
imageFrame (emptyDepthImage 1000, emptyColorImage navy)
~~~~~

There are limitations about how many images a frame can contain.
A frame can contain at most one depth image and at most one stencil image.


Pipeline construction
---------------------

Ideally, a pipeline is an arbitrary computable function which produces a frame given an input.
Unfortunately, this is not the case.
There are limitations on possible pipelines, which reflects the fact that GPUs are specialised hardware.
The current limitations are inherited from OpenGL 3.3.

Here is a step-by-step overview about how to construct pipelines.


### Constant frames

The simplest pipeline has no input and outputs a constant frame:

~~~~~ {.haskell}
makeFrame = imageFrame (emptyDepthImage 1000, emptyColorImage navy)
~~~~~


### From vertex streams to frames

After outputting a constrant frame, the next step is to construct a frame from a vertex stream.
This can be done in several phases. We go step-by-step through the phases.
Again, if these steps seem ad-hoc, think about the specialised hardware.
(We believe that the presentation of these limitiation can be impoved. We plan to gradually improve
the current interface to be more and more higher-level.)

#### From vertex streams to primitive streams: vertex transformations

In the first phase we transfrom the vertices of the input vertex stream.
Typically the vertex coordinates are projected onto the viewport and the depth (the distance of the vertex from the viewer) is calculated.
If we use [homogeneous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates), the projection and
the depth can be calculated at the same time by multiplying the vertex with a projection matrix.

The vertices usually have other attributes next to the coordinates, and these attributes can be transformed
also in the first phase.
The restriction is that at least the final vertex coordinates should be produced for rasterization.

The `transformVertices1` function transforms a vertex stream into a primitive stream when two helper functions
are given: one for calculating the vertex coordinates needed for rasterization, and the other for calculating
the transformed attributes.

The next example project and scale the vertices, and the transformed attributes will be equal to the original
coordinates of the vertices:

~~~~~ {.haskell}
transformVertices1 (scale 0.5 . (projmat *.)) id
~~~~~

Here we suppose that the vertex stream contain `Vec 4 Float` values and `projmat` has type `Mat 4 4 Float`.
How to define `projmat` is discussed later.


#### From primitive streams to fragment streams: rasterization

In the rasterization step, each primitive is turned into a set of disjunct unshaded (not-yet-colored) fragments.
A fragment is like a pixel; in fact, a pixel is an accumulated set of shaded (colored) overlapping fragments.
Accumulation means that we combine the color values of overlapping fragments to get the color of the pixel.

The rasterization step is not programmable, but configrable by a **rasterization context**.

An example rasterization context for rasterizing triangles:

~~~~~ {.haskell}
TriangleCtx CullNone PolygonFill NoOffset LastVertex
~~~~~

Some explanation (do not mind if you don't understands these now):

`CullNone` tells that both side of the triangles are visible.  
`PolygonFill` tells that not only the edges of the trianges are visible.  
`NoOffset` tells that rasterization will not modify depth values.  
`LastVertex` tells that in case of no interpolation which vertex attribute to use in fragments.

The `rasterize` functions map a rasterization context to a functions which converts a primitive stream to a fragment stream:

~~~~~ {.haskell}
rasterize (TriangleCtx CullNone PolygonFill NoOffset LastVertex)
~~~~~

#### Filtering fragment streams

It is possible to filter fragment streams. Filtering is useful to make fragments look transparent.
In the vast majority of cases we do not filter, so we write:

~~~~~ {.haskell}
filterFragmentStream PassAll
~~~~~

#### From filtered fragment streams to shaded fragment streams: shading

In the shading phase we give color to unshaded fragments.

This phase is programmable, the program is given by a function:

~~~~~ {.haskell}
transformFragmentsRastDepth (\x -> (rotMatrixZ time *. rotMatrixY time *. x) *! f time)
~~~~~

The `RastDepth` suffix tells that depth values are automatically computed by the previous rasterization depth;
we don't want to customize it.

`f` is a helper function:

~~~~~ {.haskell}
f x = (x + sin x + sin (1.1 * x)) `mod` 4 * 2
~~~~~


#### From shaded fragment streams to frames: accumulation

Now we have a shaded fragment stream which we would like to put on a frame.
For this we need a frame and we need an **accumulation context**

The accumulation context tells how to "accumulate" fragments, i.e. how to combine
previous values on the frame with new incoming values.

An example accumulation context:

~~~~~ {.haskell}
(DepthOp Less True, ColorOp NoBlending (V4 True True True True))
~~~~~

`DepthOp` tells how to accumulate depth values.  
`Less` tells that depth test succeed if the fragment's depth values is less (it is closer).  
`True` tells that the fragments are opaque, and should we written on the frame if the depth test succeeds.  
`ColorOp` tells how to accumulate (the first) color values.  
`NoBlending` tells that we don't want to blend (mix) values, the new value is written.  
`(V4 True True True True)` gives the write masks for individual color channels.

We can pair a shaded fragment stream with an accumulation context with `accumulateWith`:

~~~~~ {.haskell}
accumulateWith (DepthOp Less True, ColorOp NoBlending (V4 True True True True))
~~~~~

The actual accumulation is done with `overlay` (used as operator here):

~~~~~ {.haskell}
    = imageFrame (emptyDepthImage 1000, emptyColorImage navy)
  `overlay`
      vertexstream
    & transformVertices1 (scale 0.5 . (projmat *.)) id
    & rasterize (TriangleCtx CullNone PolygonFill NoOffset LastVertex)
    & filterFragmentStream PassAll
    & transformFragmentsRastDepth (\x -> (rotMatrixZ time *. rotMatrixY time *. x) *! f time)
    & accumulateWith (DepthOp Less True, ColorOp NoBlending (V4 True True True True))
~~~~~

Here `(&)` is the flipped function application, and `vertexstream` is a vertex stream.


### Putting all together

The pure function which creates the final frame, given the time, the projection matrix and a triangle vertex stream:

~~~~~ {.haskell}
f x = (x + sin x + sin (1.1 * x)) `mod` 4 * 2

makeFrame (time :: Float)
          (projmat :: Mat 4 4 Float)
          (vertexstream :: VertexStream Triangle (Vec 4 Float))

    = imageFrame (emptyDepthImage 1000, emptyColorImage navy)
  `overlay`
      vertexstream
    & transformVertices1 (scale 0.5 . (projmat *.)) id
    & rasterize (TriangleCtx CullNone PolygonFill NoOffset LastVertex)
    & filterFragmentStream PassAll
    & transformFragmentsRastDepth (\x -> (rotMatrixZ time *. rotMatrixY time *. x) *! f time)
    & accumulateWith (DepthOp Less True, ColorOp NoBlending (V4 True True True True))
~~~~~


Interfacing pipelines with the outer world
------------------------------------------

We can connect the inputs of the previous pipeline to our application by naming the input uniforms and input streams:

~~~~~ {.haskell}
main = renderFrame $
   makeFrame (Uniform "Time")
             (Uniform "MVP")
             (Fetch "stream4" (Attribute "position4"))
~~~~~



Getting started
===============

Suppose you want to implement the graphics engine of your application in LambdaCube 3D.

-   Decide whether you want to develop for mobile or for desktop computers.  
    Supported platforms: ...
-   Decide whether your application runs in the browser or it is a standalone application.  
    Restrictions: ...
-   Choose the language of the part of your application which communicates with the graphics engine.  
    Currently Haskell, C++ and JavaScript is supported.

After making these decisions, look at a corresponding hello world program and try to modify it.

